{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MGL870 - TP2 - Utilisation de l’apprentissage machine pour la détection des anomalies\n",
    "## Pierre Joseph, Jonathan Mésidor, Mohamed Fehd Soufi\n",
    "## Automne 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install jupyter logparser3 drain3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from logparser.Drain import LogParser\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation des donnees -utilisation de Drain3 pour parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: ./BGL/BGL_2k.log\n",
      "Total lines:  2000\n",
      "Processed 50.0% of log lines.\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.869851]\n",
      "Parsing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from logparser.Drain import LogParser\n",
    "\n",
    "input_dir = \"./BGL/\"\n",
    "output_dir = \"./result\"\n",
    "log_file = \"BGL_2k.log\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Define the log format and regular expressions\n",
    "log_format = \"<Label> <PId> <Date> <Node> <Timestamp> <NodeRepeat> <Type> <Component> <Level> <Content>\"\n",
    "regex = [\n",
    "    r\"(0x)[0-9a-fA-F]+\",  # hexadecimal\n",
    "    r\"\\d+\\.\\d+\\.\\d+\\.\\d+\",  # IP addresses\n",
    "    r\"\\d+\"  # general digits\n",
    "]\n",
    "\n",
    "# Similarity threshold and depth\n",
    "st = 0.5  # Similarity threshold\n",
    "depth = 4  # Depth of all leaf nodes\n",
    "\n",
    "# Create and execute the parser\n",
    "try:\n",
    "    parser = LogParser(log_format, indir=input_dir, outdir=output_dir, depth=depth, st=st, rex=regex)\n",
    "    parser.parse(log_file)\n",
    "    print(\"Parsing completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during parsing: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on fait un parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing log file: 2000it [00:00, 3059.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed log data saved to 'result/parsed_bgl_2k.log_structured.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Configuration and setup for TemplateMiner\n",
    "config = TemplateMinerConfig()\n",
    "drain_parser = TemplateMiner(config=config)\n",
    "\n",
    "# Define log pattern to match log fields\n",
    "log_pattern = re.compile(\n",
    "    r\"(?P<Label>\\S+)\\s\"               # Match the Label (e.g., \"-\")\n",
    "    r\"(?P<PId>\\S+)\\s\"                 # Match the Process ID (e.g., \"1117838570\")\n",
    "    r\"(?P<Date>\\d{4}\\.\\d{2}\\.\\d{2})\\s\"  # Match the Date (e.g., \"2005.06.03\")\n",
    "    r\"(?P<Node>\\S+)\\s\"                # Match the Node (e.g., \"R02-M1-N0-C:J12-U11\")\n",
    "    r\"(?P<Timestamp>\\S+)\\s\"           # Match the Timestamp (e.g., \"2005-06-03-15.42.50.675872\")\n",
    "    r\"(?P<NodeRepeat>\\S+)\\s\"          # Match the repeated Node (e.g., \"R02-M1-N0-C:J12-U11\")\n",
    "    r\"(?P<Type>\\S+)\\s\"                # Match the Type (e.g., \"RAS\")\n",
    "    r\"(?P<Component>\\S+)\\s\"           # Match the Component (e.g., \"KERNEL\")\n",
    "    r\"(?P<Level>\\S+)\\s\"               # Match the Level (e.g., \"INFO\")\n",
    "    r\"(?P<Content>.+)\"                # Match the Content (e.g., \"instruction cache parity error corrected\")\n",
    ")\n",
    "\n",
    "with open('BGL/BGL_2k.log', 'r') as log_file:\n",
    "    for line in log_file:\n",
    "        # Remove any leading/trailing whitespace from the line\n",
    "        line = line.strip()\n",
    "        match = log_pattern.match(line)\n",
    "        if match:\n",
    "            log_content = match.group(\"Content\")  # Extract the Content field\n",
    "            result = drain_parser.add_log_message(log_content)\n",
    "            #Fehd: On ne met pas le template maintenant, parce qu'à chaque ligne, il génère un arbre. Il faut faire dans deux boucles séparées, comme en bas.\n",
    "\n",
    "log_data = []\n",
    "\n",
    "# Process the log file with a progress bar\n",
    "with open('BGL/BGL_2k.log', 'r') as log_file:\n",
    "    previous_timestamp = None  # Initialize the previous timestamp for calculating DeltaT\n",
    "    for line in tqdm(log_file, desc=\"Processing log file\"):\n",
    "        line = line.strip()  # Remove leading/trailing whitespace\n",
    "        match = log_pattern.match(line)\n",
    "        if match:\n",
    "            # Extract matched fields\n",
    "            log_content = match.group(\"Content\")\n",
    "            timestamp_str = match.group(\"Timestamp\")\n",
    "            label = 1 if match.group(\"Label\") == '-' else 0  # Replace '-' with 1, others with 0\n",
    "            \n",
    "            # Convert Timestamp to Unix format\n",
    "            timestamp = pd.to_datetime(timestamp_str, format='%Y-%m-%d-%H.%M.%S.%f').value // 10**9\n",
    "            \n",
    "            # Calculate DeltaT (difference in seconds from the previous log)\n",
    "            deltaT = timestamp - previous_timestamp if previous_timestamp is not None else 0\n",
    "            previous_timestamp = timestamp  # Update previous timestamp\n",
    "            \n",
    "            # Use Drain3 to match the log entry content\n",
    "            result = drain_parser.match(log_content)\n",
    "\n",
    "            # Create a dictionary with the parsed data\n",
    "            log_entry = {\n",
    "                \"Timestamp\": timestamp,\n",
    "                \"Content\": log_content,\n",
    "                \"Matched Template\": result.get_template() if result else \"No Template\",\n",
    "                \"Cluster ID\": result.cluster_id if result else \"No Cluster\",\n",
    "                \"Label\": label,\n",
    "                \"DeltaT\": deltaT\n",
    "            }\n",
    "            \n",
    "            # Append the dictionary to the list\n",
    "            log_data.append(log_entry)\n",
    "\n",
    "# Convert log data to DataFrame and save to CSV\n",
    "df = pd.DataFrame(log_data)\n",
    "\n",
    "# Save the parsed log data to a structured CSV\n",
    "output_path = 'result/parsed_bgl_2k.log_structured.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Parsed log data saved to '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slinding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliding window results saved to 'result/parsed_bgl_2k_sliding_window.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def sliding_window(structured_log_path, para):\n",
    "    \"\"\"\n",
    "    Split logs into sliding windows/session and aggregate by log keys with labels.\n",
    "    \n",
    "    :param structured_log_path: Path to the structured log file (CSV).\n",
    "    :param para: Parameters for sliding window {\"window_size\": seconds, \"step_size\": seconds}.\n",
    "    :return: DataFrame with columns=[Timestamp, 1, 2, ..., n, Anomaly] where numbers represent log keys and their counts.\n",
    "    \"\"\"\n",
    "    # Read the structured log file\n",
    "    df_structured = pd.read_csv(structured_log_path)  # Assumes columns: Timestamp, Content, Matched Template, Cluster ID, Label, DeltaT\n",
    "    \n",
    "    # Convert Timestamp to datetime format for easier handling\n",
    "    df_structured['Timestamp'] = pd.to_datetime(df_structured['Timestamp'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
    "    \n",
    "    # Parameters\n",
    "    window_size = para[\"window_size\"]  # Window size in seconds\n",
    "    step_size = para[\"step_size\"]      # Step size in seconds\n",
    "    \n",
    "    # Initialize results\n",
    "    result = []\n",
    "    start_time = df_structured['Timestamp'].min()\n",
    "    end_time = start_time + pd.to_timedelta(window_size, unit='s')\n",
    "    \n",
    "    while start_time <= df_structured['Timestamp'].max():\n",
    "        # Filter logs within the current window\n",
    "        window_logs = df_structured[(df_structured['Timestamp'] >= start_time) & (df_structured['Timestamp'] < end_time)]\n",
    "        \n",
    "        if not window_logs.empty:\n",
    "            # Count log keys (Cluster ID)\n",
    "            log_key_counts = window_logs['Cluster ID'].value_counts().to_dict()\n",
    "            \n",
    "            # Aggregate anomaly labels (1 if any anomaly exists in the window, else 0)\n",
    "            anomaly_label = int(window_logs['Label'].max())\n",
    "            \n",
    "            # Create a row with timestamp, log key counts, and anomaly label\n",
    "            row = {\n",
    "                'Timestamp': start_time,\n",
    "                **{str(key): log_key_counts.get(key, 0) for key in range(1, len(df_structured['Cluster ID'].unique()) + 1)},\n",
    "                'Anomaly': anomaly_label\n",
    "            }\n",
    "            result.append(row)\n",
    "        \n",
    "        # Slide the window\n",
    "        start_time += pd.to_timedelta(step_size, unit='s')\n",
    "        end_time = start_time + pd.to_timedelta(window_size, unit='s')\n",
    "    \n",
    "    # Convert result to DataFrame and fill missing log keys with 0\n",
    "    result_df = pd.DataFrame(result).fillna(0)\n",
    "    return result_df\n",
    "\n",
    "# Parameters for sliding window (5-minute window with 30-second step)\n",
    "window_size = 5 * 60  # 5 minutes in seconds\n",
    "step_size = 30  # 30 seconds in seconds\n",
    "\n",
    "# Generate sliding window dataset\n",
    "deeplog_df = sliding_window(\n",
    "    \"result/parsed_bgl_2k.log_structured.csv\",\n",
    "    para={'window_size': window_size, 'step_size': step_size}\n",
    ")\n",
    "\n",
    "# Save the result to CSV\n",
    "deeplog_df.to_csv(\"result/parsed_bgl_2k_sliding_window.csv\", index=False)\n",
    "\n",
    "print(\"Sliding window results saved to 'result/parsed_bgl_2k_sliding_window.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
