{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MGL870 - TP2 - Utilisation de l’apprentissage machine pour la détection des anomalies\n",
    "## Pierre Joseph, Jonathan Mésidor, Mohamed Fehd Soufi\n",
    "## Automne 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from logparser.Drain import LogParser\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split log file into train-valid-test (60-20-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split_logfile(input_file, train_file, valid_file, test_file, train_ratio=0.6, valid_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a file into train, validation, and test sets based on the given ratios.\n",
    "\n",
    "    Parameters:\n",
    "    input_file (str): Path to the input file.\n",
    "    train_file (str): Path to the output train file.\n",
    "    valid_file (str): Path to the output validation file.\n",
    "    test_file (str): Path to the output test file.\n",
    "    train_ratio (float): Ratio of the train set size to the total size (default is 0.6).\n",
    "    valid_ratio (float): Ratio of the validation set size to the total size (default is 0.2).\n",
    "    \"\"\"\n",
    "    # Read the input file\n",
    "    with open(input_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_split_index = int(len(lines) * train_ratio)\n",
    "    valid_split_index = int(len(lines) * (train_ratio + valid_ratio))\n",
    "\n",
    "    # Split the lines into train, validation, and test sets\n",
    "    train_lines = lines[:train_split_index]\n",
    "    valid_lines = lines[train_split_index:valid_split_index]\n",
    "    test_lines = lines[valid_split_index:]\n",
    "\n",
    "    # Write the train set to the train file\n",
    "    with open(train_file, 'w') as file:\n",
    "        file.writelines(train_lines)\n",
    "\n",
    "    # Write the validation set to the validation file\n",
    "    with open(valid_file, 'w') as file:\n",
    "        file.writelines(valid_lines)\n",
    "\n",
    "    # Write the test set to the test file\n",
    "    with open(test_file, 'w') as file:\n",
    "        file.writelines(test_lines)\n",
    "\n",
    "    print(f\"Split completed: \\n {len(train_lines)} lines in {train_file} \\n {len(valid_lines)} lines in {valid_file} \\n {len(test_lines)} lines in {test_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_logfile(input_file='input/HDFS_v1/HDFS.log', \n",
    "              train_file='input/HDFS_v1/HDFS_train.log', \n",
    "              valid_file='input/HDFS_v1/HDFS_valid.log', \n",
    "              test_file='input/HDFS_v1/HDFS_test.log')\n",
    "\n",
    "#Split completed: \n",
    "# 6705377 lines in input/HDFS_v1/HDFS_train.log \n",
    "# 2235126 lines in input/HDFS_v1/HDFS_valid.log \n",
    "# 2235126 lines in input/HDFS_v1/HDFS_test.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drain 3 parser on HDFS_train.log, HDFS_valid.log and HDFS_test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir  = 'input/HDFS_v1/'  # The input directory of log files\n",
    "output_dir = 'output/'  # The output directory of parsing results\n",
    "log_files  = ['HDFS_train.log', 'HDFS_valid.log', 'HDFS_test.log']  # List of input log file names\n",
    "log_format = '<Date> <Time> <Pid> <Level> <Component>: <Content>'  # HDFS log format\n",
    "\n",
    "# Regular expression list for optional preprocessing (default: [])\n",
    "regex = [\n",
    "    r'blk_(|-)[0-9]+',  # block id\n",
    "    r'(/|)([0-9]+\\.){3}[0-9]+(:[0-9]+|)(:|)',  # IP\n",
    "    r'(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[^A-Za-z0-9])|[0-9]+$',  # Numbers\n",
    "]\n",
    "st = 0.5  # Similarity threshold\n",
    "depth = 4  # Depth of all leaf nodes\n",
    "\n",
    "parser = LogParser(log_format, indir=input_dir, outdir=output_dir, depth=depth, st=st, rex=regex)\n",
    "\n",
    "# Iterate over each log file and parse it\n",
    "for log_file in log_files:\n",
    "    parser.parse(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "We created the EventId mapping to have a clearer matrix of events. First we sort the EventId by order of magnitude then we associate a value E(x) = {E0, E1, E2 .... } to each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(file_names, output_dir):\n",
    "    \"\"\"\n",
    "    Process multiple log template CSV files and save the mappings to JSON files in a specified output directory.\n",
    "    \n",
    "    Parameters:\n",
    "        file_names (list): A list of CSV file names to process.\n",
    "        output_dir (str): The directory where output JSON files will be saved.\n",
    "    \"\"\"\n",
    "    # Assure that the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        log_templates_file = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        # Lire le fichier CSV et créer le dictionnaire de mappage\n",
    "        log_temp = pd.read_csv(log_templates_file).sort_values(by=\"Occurrences\", ascending=False)\n",
    "        log_temp_dict = {event: f\"E{idx + 1}\" for idx, event in enumerate(log_temp[\"EventId\"])}\n",
    "        \n",
    "        # Définir le nom du fichier de sortie\n",
    "        output_file = os.path.join(output_dir, f\"{file_name.replace('.csv', '')}.json\")\n",
    "        \n",
    "        # Sauvegarder le dictionnaire de mappage\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(log_temp_dict, f)\n",
    "        \n",
    "        print(f\"Mapping completed and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"HDFS_train.log_templates.csv\",\n",
    "    \"HDFS_valid.log_templates.csv\",\n",
    "    \"HDFS_test.log_templates.csv\"\n",
    "]\n",
    "output_directory = \"./output/\"\n",
    "\n",
    "mapping(files, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the structure of HDFS to have Blk\n",
    "Adding the columns BlockId, Label and Change the EventId to E1, E2, E3 ... so that we can have a complete structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_files(input_dir, output_dir, json_filename, structured_log_filename, anomaly_label_filename, output_filename):\n",
    "    \"\"\"\n",
    "    Process log files to add BlockId, map EventId values, and label anomalies.\n",
    "    \n",
    "    Parameters:\n",
    "        input_dir (str): The directory containing the input files.\n",
    "        output_dir (str): The directory for output files.\n",
    "        json_filename (str): The JSON file with EventId mappings.\n",
    "        structured_log_filename (str): The CSV file with structured log data.\n",
    "        anomaly_label_filename (str): The CSV file with anomaly labels.\n",
    "        output_filename (str): The filename for the processed output.\n",
    "    \"\"\"\n",
    "    # Paths for the input files\n",
    "    json_file_path = os.path.join(output_dir, json_filename)\n",
    "    anomaly_label_path = os.path.join(input_dir, anomaly_label_filename)\n",
    "    structured_log_path = os.path.join(output_dir, structured_log_filename)\n",
    "    \n",
    "    # Load the structured log file\n",
    "    df_structured = pd.read_csv(structured_log_path)\n",
    "\n",
    "    # Load the JSON mapping file\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        event_mapping = json.load(json_file)\n",
    "\n",
    "    # Load the anomaly label file\n",
    "    df_labels = pd.read_csv(anomaly_label_path)\n",
    "    df_labels['Label'] = df_labels['Label'].replace({'Normal': 'Success', 'Anomaly': 'Fail'})\n",
    "\n",
    "    # Add BlockId by extracting block identifiers\n",
    "    df_structured['BlockId'] = df_structured['Content'].apply(lambda x: re.search(r'blk_(|-)[0-9]+', x).group(0) if re.search(r'blk_(|-)[0-9]+', x) else None)\n",
    "\n",
    "    # Remove rows where BlockId is NaN\n",
    "    df_structured = df_structured.dropna(subset=['BlockId'])\n",
    "\n",
    "    # Map EventId using the JSON file\n",
    "    df_structured['EventId'] = df_structured['EventId'].apply(lambda x: event_mapping.get(x, x))\n",
    "\n",
    "    # Merge DataFrames to add the Label column\n",
    "    df_structured = pd.merge(df_structured, df_labels, on='BlockId', how='left')\n",
    "\n",
    "    # Reorder columns so BlockId and Label are first\n",
    "    columns = ['BlockId', 'Label'] + [col for col in df_structured.columns if col not in ['BlockId', 'Label']]\n",
    "    df_structured = df_structured[columns]\n",
    "\n",
    "    # Save the processed structured log file\n",
    "    output_path = os.path.join(output_filename)\n",
    "    df_structured.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Le fichier structuré avec BlockId et les EventId remplacés est généré et sauvegardé dans {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "process_log_files(\n",
    "    input_dir='./input/HDFS_v1/',\n",
    "    output_dir='./output',\n",
    "    json_filename='HDFS_train.log_templates.json',\n",
    "    structured_log_filename='HDFS_train.log_structured.csv',\n",
    "    anomaly_label_filename='preprocessed/anomaly_label.csv',\n",
    "    output_filename='HDFS_train.log_structured_blk.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid\n",
    "process_log_files(\n",
    "    input_dir='./input/HDFS_v1/',\n",
    "    output_dir='./output',\n",
    "    json_filename='HDFS_valid.log_templates.json',\n",
    "    structured_log_filename='HDFS_valid.log_structured.csv',\n",
    "    anomaly_label_filename='preprocessed/anomaly_label.csv',\n",
    "    output_filename='HDFS_valid.log_structured_blk.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "process_log_files(\n",
    "    input_dir='./input/HDFS_v1/',\n",
    "    output_dir='./output',\n",
    "    json_filename='HDFS_test.log_templates.json',\n",
    "    structured_log_filename='HDFS_test.log_structured.csv',\n",
    "    anomaly_label_filename='preprocessed/anomaly_label.csv',\n",
    "    output_filename='HDFS_test.log_structured_blk.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdfs_sampling(file_names, input_dir, output_dir, window='session', window_size=0):\n",
    "    \"\"\"\n",
    "    Perform HDFS sampling on multiple structured log files.\n",
    "\n",
    "    Parameters:\n",
    "        file_names (list): A list of file names to process.\n",
    "        input_dir (str): The directory containing the input structured log files.\n",
    "        output_dir (str): The directory where output files will be saved.\n",
    "        window (str): The type of windowing (default is 'session').\n",
    "        window_size (int): The size of the window (not used for session windowing).\n",
    "    \"\"\"\n",
    "    assert window == 'session', \"Only window=session is supported for HDFS dataset.\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        input_path = os.path.join(input_dir, file_name)\n",
    "        output_path = os.path.join(output_dir, file_name.replace('.csv', '_sequence.csv'))\n",
    "\n",
    "        print(\"Loading\", input_path)\n",
    "\n",
    "        # Load the structured log file with optimized data types for large files\n",
    "        struct_log = pd.read_csv(input_path, engine='c', na_filter=False, memory_map=True, dtype={'Time': str})\n",
    "        \n",
    "        # Standardize the Time format by adding leading zeros for HHMMSS format\n",
    "        struct_log['Time'] = struct_log['Time'].str.zfill(6)\n",
    "\n",
    "        # Pad Date values with zeros to ensure uniform length of 6\n",
    "        struct_log['Date'] = struct_log['Date'].astype(str).str.zfill(6)\n",
    "        \n",
    "        # Extract BlockId, fill EventId missing values, and map Label to binary\n",
    "        struct_log['BlockId'] = struct_log['Content'].str.extract(r'(blk_-?\\d+)')\n",
    "        struct_log['EventId'] = struct_log['EventId'].fillna('')\n",
    "        struct_log['Label'] = struct_log['Label'].apply(lambda x: 1 if x == 'Fail' else 0)\n",
    "\n",
    "        # Initialize dictionaries to store results\n",
    "        data_dict = defaultdict(list)\n",
    "        time_dict = defaultdict(list)\n",
    "        date_dict = defaultdict(list)\n",
    "        type_count = defaultdict(int)\n",
    "\n",
    "        # Group by BlockId for bulk operations\n",
    "        grouped = struct_log.groupby('BlockId')\n",
    "        for block_id, group in tqdm(grouped, total=len(grouped)):\n",
    "            data_dict[block_id] = group['EventId'].tolist()\n",
    "            time_dict[block_id] = pd.to_datetime(group['Time'], format='%H%M%S', errors='coerce').dropna()\n",
    "            date_dict[block_id] = group['Date'].tolist()\n",
    "            type_count[block_id] = group['Label'].sum()  # Count occurrences of \"Fail\"\n",
    "\n",
    "        # Build the final DataFrame\n",
    "        rows = []\n",
    "        for block_id, events in tqdm(data_dict.items(), total=len(data_dict)):\n",
    "            features = [event for event in events if event]\n",
    "\n",
    "            times = time_dict[block_id]\n",
    "            dates = date_dict[block_id]\n",
    "            if len(times) > 1:\n",
    "                time_intervals = [(times.iloc[i] - times.iloc[i - 1]).total_seconds() for i in range(1, len(times))]\n",
    "                latency = (times.iloc[-1] - times.iloc[0]).total_seconds()\n",
    "            else:\n",
    "                time_intervals = []\n",
    "                latency = 0\n",
    "\n",
    "            label = 'Fail' if type_count[block_id] > 0 else 'Success'\n",
    "\n",
    "            # Use the first occurrence of Date and Time as representative for each BlockId\n",
    "            first_date = dates[0] if dates else ''\n",
    "            first_time = times.iloc[0].strftime('%H%M%S') if not times.empty else ''\n",
    "\n",
    "            rows.append({\n",
    "                \"BlockId\": block_id,\n",
    "                \"Label\": label,\n",
    "                \"Type\": type_count[block_id],\n",
    "                \"Features\": str(features),\n",
    "                \"Date\": first_date,\n",
    "                \"Time\": first_time,\n",
    "                \"TimeInterval\": str(time_intervals),\n",
    "                \"Latency\": latency\n",
    "            })\n",
    "\n",
    "        data_df = pd.DataFrame(rows, columns=['BlockId', 'Label', 'Type', 'Features', 'Date', 'Time', 'TimeInterval', 'Latency'])\n",
    "        data_df.to_csv(output_path, index=False)\n",
    "        print(f\"HDFS sampling completed. Output saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'HDFS_train.log_structured_blk.csv',\n",
    "    'HDFS_valid.log_structured_blk.csv',\n",
    "    'HDFS_test.log_structured_blk.csv'\n",
    "]\n",
    "hdfs_sampling(files, input_dir='output/', output_dir='output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Occurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_event_occurrence_matrix(log_files, event_traces_files, input_dir, output_dir, event_columns=None):\n",
    "    \"\"\"\n",
    "    Generate event occurrence matrices for multiple log files.\n",
    "\n",
    "    Parameters:\n",
    "        log_files (list): A list of log file names (e.g., ['HDFS_train.log', 'HDFS_valid.log']).\n",
    "        event_traces_files (list): A list of corresponding event trace CSV files (e.g., ['output/HDFS_train.log_structured_blk_sequence.csv']).\n",
    "        input_dir (str): The directory where the log files and other inputs are located.\n",
    "        output_dir (str): The directory where the output CSV files should be saved.\n",
    "        event_columns (list): List of event columns (e.g., ['E1', 'E2', ..., 'E29']). If None, defaults to ['E1' to 'E29'].\n",
    "    \"\"\"\n",
    "    if event_columns is None:\n",
    "        event_columns = [f\"E{i}\" for i in range(1, 30)]  # Default event columns (E1 to E29)\n",
    "\n",
    "    # Path to the anomaly label file\n",
    "    anomaly_label_file = os.path.join(input_dir, \"preprocessed/anomaly_label.csv\")\n",
    "\n",
    "    # Load anomaly labels and map them (only once)\n",
    "    anomaly_labels = pd.read_csv(anomaly_label_file)\n",
    "    anomaly_labels['Label'] = anomaly_labels['Label'].apply(lambda x: 'Fail' if x == 'Anomaly' else 'Success')\n",
    "    label_dict = anomaly_labels.set_index('BlockId')['Label'].to_dict()\n",
    "\n",
    "    # Iterate over each log file and corresponding event trace file\n",
    "    for log_file, event_traces_file in zip(log_files, event_traces_files):\n",
    "        output_file = os.path.join(output_dir, f\"Event_occurence_matrix_{log_file.replace('.log', '')}.csv\")\n",
    "        print(f\"Processing {log_file}...\")\n",
    "\n",
    "        # Load the corresponding event traces for the current log file\n",
    "        event_traces = pd.read_csv(event_traces_file)\n",
    "\n",
    "        # Initialize occurrence matrix for each log file\n",
    "        occurrence_matrix = []\n",
    "\n",
    "        # Iterate over each row to build the occurrence matrix\n",
    "        for _, row in event_traces.iterrows():\n",
    "            block_id = row['BlockId']\n",
    "            label = label_dict.get(block_id, 'Unknown')\n",
    "            features = row['Features']\n",
    "            event_list = re.findall(r\"E\\d+\", features)\n",
    "\n",
    "            # Count occurrences of each event\n",
    "            event_counts = {event: event_list.count(event) for event in event_columns}\n",
    "            \n",
    "            # Add each entry to the occurrence matrix with Time and Date\n",
    "            occurrence_matrix.append({\n",
    "                \"BlockId\": block_id,\n",
    "                \"Label\": label,\n",
    "                \"Type\": int(row['Type']) if pd.notna(row['Type']) else 0,  # Ensure integer for Type\n",
    "                \"Time\": row['Time'] if 'Time' in row else '',\n",
    "                \"Date\": row['Date'] if 'Date' in row else '',\n",
    "                **event_counts\n",
    "            })\n",
    "\n",
    "        # Convert to DataFrame and save\n",
    "        occurrence_matrix_df = pd.DataFrame(occurrence_matrix)\n",
    "        occurrence_matrix_df = occurrence_matrix_df[['BlockId', 'Label', 'Type', 'Time', 'Date'] + event_columns]\n",
    "        occurrence_matrix_df.to_csv(output_file, index=False)\n",
    "        print(f\"Event occurrence matrix for {log_file} saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_files = ['HDFS_train.log', 'HDFS_valid.log', 'HDFS_test.log']\n",
    "event_traces_files = [\n",
    "    'output/HDFS_train.log_structured_blk_sequence.csv',\n",
    "    'output/HDFS_valid.log_structured_blk_sequence.csv',\n",
    "    'output/HDFS_test.log_structured_blk_sequence.csv'\n",
    "]\n",
    "input_dir = './input/HDFS_v1/'\n",
    "output_dir = './output/'\n",
    "\n",
    "generate_event_occurrence_matrix(log_files, event_traces_files, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Date and Time variables to Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date      Time\n",
      "0 2010-08-11  02:01:06\n",
      "1 2010-08-11  01:51:17\n",
      "2 2010-08-11  22:14:31\n",
      "3 2010-08-11  01:24:44\n",
      "4 2009-08-11  21:15:24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert Time column (e.g., 20106 to 20:10:06)\n",
    "train_data['Time'] = train_data['Time'].apply(lambda x: f\"{x:06d}\")  # Ensure it's 6 digits\n",
    "train_data['Time'] = pd.to_datetime(train_data['Time'], format='%H%M%S').dt.time\n",
    "\n",
    "# Convert Date column (e.g., 81110 to 2010-08-11)\n",
    "train_data['Date'] = train_data['Date'].apply(lambda x: f\"{x:05d}\")  # Ensure it's 5 digits\n",
    "train_data['Date'] = pd.to_datetime(train_data['Date'], format='%m%d%y')\n",
    "\n",
    "# Save the modified data to a new CSV file\n",
    "train_data.to_csv('output/Event_occurence_matrix_HDFS_train_updated.csv', index=False)\n",
    "\n",
    "print(train_data[['Date', 'Time']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
