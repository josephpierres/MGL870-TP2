{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "Importer les bibliothèques nécessaires, y compris pandas, numpy, scikit-learn, et matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn style for plots\n",
    "#sns.set(style=\"whitegrid\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')  # Appliquer le style ggplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données\n",
    "Charger les fichiers CSV HDFS_occurence_matrix_HDFS_train.csv, HDFS_occurence_matrix_HDFS_valid.csv, et HDFS_occurence_matrix_HDFS_test.csv dans des DataFrames pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Timestamp    E1  E2  E3  E4  E5  E6  E7  E8  E9  ...  E499  E500  \\\n",
      "0  2005-06-03 15:42:50  1625   1   0   0   0   0   0   0   0  ...     0     0   \n",
      "1  2005-06-03 15:43:20  1640   1   0   0   0   0   0   0   0  ...     0     0   \n",
      "2  2005-06-03 15:43:50  1637   1   0   0   0   0   0   0   0  ...     0     0   \n",
      "3  2005-06-03 15:44:20  1632   1   0   0   0   0   0   0   0  ...     0     0   \n",
      "4  2005-06-03 15:44:50  1593   1   0   0   0   0   0   0   0  ...     0     0   \n",
      "\n",
      "   E501  E502  E503  E504  E505  E506  E507  Anomaly  \n",
      "0     0     0     0     0     0     0     0     Fail  \n",
      "1     0     0     0     0     0     0     0     Fail  \n",
      "2     0     0     0     0     0     0     0     Fail  \n",
      "3     0     0     0     0     0     0     0     Fail  \n",
      "4     0     0     0     0     0     0     0     Fail  \n",
      "\n",
      "[5 rows x 509 columns]\n",
      "             Timestamp  E1  E2    E3  E4  E5  E6  E7  E8  E9  ...  E657  E658  \\\n",
      "0  2005-07-28 12:16:21   0   0  4879   0   0   0   0   0   0  ...     0     0   \n",
      "1  2005-07-28 12:16:51   0   0  3824   0   0   0   0   0   0  ...     0     0   \n",
      "2  2005-07-28 12:17:21   0   0  2744   0   0   0   0   0   0  ...     0     0   \n",
      "3  2005-07-28 12:17:51   0   0  1671   0   0   0   0   0   0  ...     0     0   \n",
      "4  2005-07-28 12:18:21   0   0   596   0   0   0   0   0   0  ...     0     0   \n",
      "\n",
      "   E659  E660  E661  E662  E663  E664  E665  Anomaly  \n",
      "0     0     0     0     0     0     0     0     Fail  \n",
      "1     0     0     0     0     0     0     0     Fail  \n",
      "2     0     0     0     0     0     0     0     Fail  \n",
      "3     0     0     0     0     0     0     0     Fail  \n",
      "4     0     0     0     0     0     0     0     Fail  \n",
      "\n",
      "[5 rows x 667 columns]\n",
      "             Timestamp  E1  E2  E3  E4  E5  E6  E7  E8  E9  ...  E1035  E1036  \\\n",
      "0  2005-11-03 15:23:59   0   0   0   0   0   0   0   0   0  ...      0      0   \n",
      "1  2005-11-03 15:24:29   0   0   0   0   0   0   0   0   0  ...      0      0   \n",
      "2  2005-11-03 15:24:59   0   0   0   0   0   0   0   0   0  ...      0      0   \n",
      "3  2005-11-03 15:25:29   0   0   0   0   0   0   0   0   0  ...      0      0   \n",
      "4  2005-11-03 15:25:59   0   0   0   0   0   0   0   0   0  ...      0      0   \n",
      "\n",
      "   E1037  E1038  E1039  E1040  E1041  E1042  E1043  Anomaly  \n",
      "0      0      0      0      0      0      0      0     Fail  \n",
      "1      0      0      0      0      0      0      0     Fail  \n",
      "2      0      0      0      0      0      0      0     Fail  \n",
      "3      0      0      0      0      0      0      0     Fail  \n",
      "4      0      0      0      0      0      0      0     Fail  \n",
      "\n",
      "[5 rows x 1045 columns]\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données\n",
    "train_df = pd.read_csv('BGL_results/BGL_train_sliding_window.csv')\n",
    "valid_df = pd.read_csv('BGL_results/BGL_valid_sliding_window.csv')\n",
    "test_df = pd.read_csv('BGL_results/BGL_test_sliding_window.csv')\n",
    "\n",
    "# Afficher les premières lignes des DataFrames pour vérifier le chargement\n",
    "print(train_df.head())\n",
    "print(valid_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des variables et traitement des corrélations\n",
    "Calculer la matrice de corrélation sur les variables indépendantes dans l'ensemble d'entraînement. Identifier et supprimer les variables fortement corrélées (corrélation > 0,9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des colonnes non indépendantes et calcul de la matrice de corrélation\n",
    "correlation_matrix = train_df.drop(columns=['Timestamp', 'Anomaly']).corr()\n",
    "\n",
    "# Affichage de la matrice de corrélation avec des ajustements pour éviter le chevauchement\n",
    "plt.figure(figsize=(14, 12))  # Augmenter la taille du graphique\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap='coolwarm', \n",
    "    square=True, \n",
    "    annot_kws={\"size\": 8}  # Taille des annotations\n",
    ")\n",
    "\n",
    "# Ajuster l'alignement des étiquettes des axes\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotation des étiquettes sur l'axe x\n",
    "plt.yticks(rotation=0, fontsize=10)  # Rotation des étiquettes sur l'axe y\n",
    "\n",
    "plt.title('Matrice de corrélation des variables indépendantes', fontsize=14)\n",
    "plt.tight_layout()  # Ajuste automatiquement les marges pour éviter les chevauchements\n",
    "plt.show()\n",
    "\n",
    "# Identification des variables fortement corrélées (corrélation > 0.9)\n",
    "high_corr_var = set()\n",
    "threshold = 0.9\n",
    "\n",
    "# Parcourir les éléments de la matrice de corrélation\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):  # Éviter les doublons et la diagonale\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            high_corr_var.add(colname)\n",
    "\n",
    "# Suppression des variables fortement corrélées\n",
    "train_df_reduced = train_df.drop(columns=high_corr_var)\n",
    "valid_df_reduced = valid_df.drop(columns=high_corr_var)\n",
    "test_df_reduced = test_df.drop(columns=high_corr_var)\n",
    "\n",
    "# Affichage des variables supprimées\n",
    "print(f\"Variables supprimées en raison d'une forte corrélation: {high_corr_var}\")\n",
    "\n",
    "# Vérification des DataFrames réduits\n",
    "print(\"Aperçu des DataFrames réduits :\")\n",
    "print(train_df_reduced.head())\n",
    "print(valid_df_reduced.head())\n",
    "print(test_df_reduced.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création des échantillons avec bootstrap\n",
    "Utiliser le bootstrap pour créer des échantillons de l'ensemble d'entraînement. Chaque échantillon a la même taille que l'ensemble d'entraînement d'origine (avec remplacement).\n",
    "\n",
    "Pour chaque échantillon bootstrap, entraîner un modèle de régression logistique sur l'ensemble d'entraînement. Valider ce modèle sur un ensemble de validation distinct.\n",
    "\n",
    "Répéter les étapes de création d'échantillons bootstrap et de modélisation un nombre défini de fois. Stocker les performances (Précision, Rappel et AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Définir le nombre d'échantillons bootstrap ou répétitions\n",
    "n_iterations = 100\n",
    "\n",
    "# Préparer les données\n",
    "label_mapping = {'Fail': 1, 'Success': 0}\n",
    "train_df_reduced['Anomaly'] = train_df_reduced['Anomaly'].map(label_mapping)\n",
    "valid_df_reduced['Anomaly'] = valid_df_reduced['Anomaly'].map(label_mapping)\n",
    "\n",
    "X_train = train_df_reduced.drop(columns=['Timestamp', 'Anomaly'])\n",
    "y_train = train_df_reduced['Anomaly']\n",
    "X_valid = valid_df_reduced.drop(columns=['Timestamp', 'Anomaly'])\n",
    "y_valid = valid_df_reduced['Anomaly']\n",
    "\n",
    "# Initialiser les listes pour stocker les performances des modèles\n",
    "metrics_lr = {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": [], \"roc_auc\": []}\n",
    "metrics_rf = {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": [], \"roc_auc\": []}\n",
    "\n",
    "# Boucle principale pour le bootstrap et la modélisation\n",
    "for _ in range(n_iterations):\n",
    "    # Créer un échantillon bootstrap\n",
    "    bootstrap_sample = resample(train_df_reduced, replace=True, n_samples=len(train_df_reduced), random_state=None)\n",
    "    X_train_bootstrap = bootstrap_sample.drop(columns=['Timestamp', 'Anomaly'])\n",
    "    y_train_bootstrap = bootstrap_sample['Anomaly']\n",
    "    \n",
    "    # Entraîner Logistic Regression\n",
    "    model_lr = LogisticRegression(solver='saga', max_iter=5000, random_state=None)\n",
    "    model_lr.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "    y_pred_lr = model_lr.predict(X_valid)\n",
    "    y_pred_proba_lr = model_lr.predict_proba(X_valid)[:, 1]\n",
    "    \n",
    "    # Calculer les métriques pour Logistic Regression\n",
    "    metrics_lr[\"accuracy\"].append(accuracy_score(y_valid, y_pred_lr))\n",
    "    metrics_lr[\"precision\"].append(precision_score(y_valid, y_pred_lr))\n",
    "    metrics_lr[\"recall\"].append(recall_score(y_valid, y_pred_lr))\n",
    "    metrics_lr[\"f1\"].append(f1_score(y_valid, y_pred_lr))\n",
    "    metrics_lr[\"roc_auc\"].append(roc_auc_score(y_valid, y_pred_proba_lr))\n",
    "    \n",
    "    # Entraîner Random Forest\n",
    "    model_rf = RandomForestClassifier(n_estimators=100, random_state=None)\n",
    "    model_rf.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "    y_pred_rf = model_rf.predict(X_valid)\n",
    "    y_pred_proba_rf = model_rf.predict_proba(X_valid)[:, 1]\n",
    "    \n",
    "    # Calculer les métriques pour Random Forest\n",
    "    metrics_rf[\"accuracy\"].append(accuracy_score(y_valid, y_pred_rf))\n",
    "    metrics_rf[\"precision\"].append(precision_score(y_valid, y_pred_rf))\n",
    "    metrics_rf[\"recall\"].append(recall_score(y_valid, y_pred_rf))\n",
    "    metrics_rf[\"f1\"].append(f1_score(y_valid, y_pred_rf))\n",
    "    metrics_rf[\"roc_auc\"].append(roc_auc_score(y_valid, y_pred_proba_rf))\n",
    "\n",
    "# Afficher les performances moyennes pour Logistic Regression\n",
    "print(\"\\nPerformances moyennes pour Logistic Regression:\")\n",
    "for metric, values in metrics_lr.items():\n",
    "    print(f\"{metric.capitalize()} moyenne: {np.mean(values):.4f}\")\n",
    "\n",
    "# Afficher les performances moyennes pour Random Forest\n",
    "print(\"\\nPerformances moyennes pour Random Forest:\")\n",
    "for metric, values in metrics_rf.items():\n",
    "    print(f\"{metric.capitalize()} moyenne: {np.mean(values):.4f}\")\n",
    "\n",
    "# Visualisation des performances\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "metrics_names = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]\n",
    "titles = ['Accuracy', 'Précision', 'Rappel', 'F1-Score', 'AUC-ROC']\n",
    "\n",
    "for i, metric in enumerate(metrics_names):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    sns.boxplot(data=metrics_lr[metric])\n",
    "    plt.title(f'Logistic Regression: {titles[i]}')\n",
    "    \n",
    "    plt.subplot(2, 5, i + 6)\n",
    "    sns.boxplot(data=metrics_rf[metric])\n",
    "    plt.title(f'Random Forest: {titles[i]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluation finale sur le test\n",
    "Appliquer le meilleur modèle sur l'ensemble de test. Calculer la matrice de confusion et des métriques comme l'accuracy, la précision, le rappel, le F1-Score, et l'AUC-ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Combine training and validation sets\n",
    "X_train_full = pd.concat([X_train, X_valid])\n",
    "y_train_full = pd.concat([y_train, y_valid])\n",
    "\n",
    "# Prepare test set features and target\n",
    "X_test = test_df_reduced.drop(columns=['Timestamp', 'Anomaly'])\n",
    "y_test = test_df_reduced['Anomaly']\n",
    "\n",
    "# Convert y_test to numerical values\n",
    "label_mapping = {'Fail': 1, 'Success': 0}\n",
    "y_test = y_test.map(label_mapping)\n",
    "\n",
    "# Handle missing values with imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_full = pd.DataFrame(imputer.fit_transform(X_train_full), columns=X_train_full.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# Verify the conversion\n",
    "if y_test.isnull().any():\n",
    "    raise ValueError(\"y_test contains NaN values after mapping. Check the data.\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Store performance metrics\n",
    "metrics = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- Modèle : {model_name} ---\")\n",
    "    # Train the model\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "    precision_test = precision_score(y_test, y_test_pred)\n",
    "    recall_test = recall_score(y_test, y_test_pred)\n",
    "    f1_test = f1_score(y_test, y_test_pred)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "    # Store metrics\n",
    "    metrics[model_name] = {\n",
    "        \"Accuracy\": accuracy_test,\n",
    "        \"Precision\": precision_test,\n",
    "        \"Recall\": recall_test,\n",
    "        \"F1-Score\": f1_test,\n",
    "        \"AUC-ROC\": roc_auc_test\n",
    "    }\n",
    "\n",
    "    # Display performance metrics\n",
    "    print(f\"Accuracy sur le test: {accuracy_test:.4f}\")\n",
    "    print(f\"Précision sur le test: {precision_test:.4f}\")\n",
    "    print(f\"Rappel sur le test: {recall_test:.4f}\")\n",
    "    print(f\"F1-Score sur le test: {f1_test:.4f}\")\n",
    "    print(f\"AUC-ROC sur le test: {roc_auc_test:.4f}\")\n",
    "\n",
    "    # Calculate and display the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap='Blues', xticklabels=['Success', 'Fail'], yticklabels=['Success', 'Fail'])\n",
    "    plt.xlabel('Prédiction')\n",
    "    plt.ylabel('Réalité')\n",
    "    plt.title(f'Matrice de confusion - {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC-ROC = {roc_auc_test:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "    plt.xlabel('Taux de faux positifs (FPR)')\n",
    "    plt.ylabel('Taux de vrais positifs (TPR)')\n",
    "    plt.title(f'Courbe ROC - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Compare the performance metrics of the two models\n",
    "print(\"\\n--- Comparaison des performances ---\")\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "print(metrics_df)\n",
    "\n",
    "# Visualize the comparison as a bar chart\n",
    "metrics_df.plot(kind='bar', figsize=(12, 6), rot=0)\n",
    "plt.title(\"Comparaison des performances des modèles\")\n",
    "plt.ylabel(\"Valeurs des métriques\")\n",
    "plt.xlabel(\"Modèles\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Déterminer le meilleur modèle\n",
    "Les performances des deux modèles sont affichées avec des valeurs pour chaque métrique.\n",
    "\n",
    "Le modèle avec la meilleure performance globale est identifié et assigné à best_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_full, y_train_full)\n",
    "y_test_pred_lr = lr_model.predict(X_test)\n",
    "y_test_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_full, y_train_full)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "y_test_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculer les métriques pour Logistic Regression\n",
    "lr_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_test_pred_lr),\n",
    "    \"Precision\": precision_score(y_test, y_test_pred_lr),\n",
    "    \"Recall\": recall_score(y_test, y_test_pred_lr),\n",
    "    \"F1-Score\": f1_score(y_test, y_test_pred_lr),\n",
    "    \"AUC-ROC\": roc_auc_score(y_test, y_test_pred_proba_lr)\n",
    "}\n",
    "\n",
    "# Calculer les métriques pour Random Forest\n",
    "rf_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_test_pred_rf),\n",
    "    \"Precision\": precision_score(y_test, y_test_pred_rf),\n",
    "    \"Recall\": recall_score(y_test, y_test_pred_rf),\n",
    "    \"F1-Score\": f1_score(y_test, y_test_pred_rf),\n",
    "    \"AUC-ROC\": roc_auc_score(y_test, y_test_pred_proba_rf)\n",
    "}\n",
    "\n",
    "# Comparer les métriques\n",
    "print(\"\\nPerformances de Logistic Regression:\")\n",
    "for metric, value in lr_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nPerformances de Random Forest:\")\n",
    "for metric, value in rf_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Décider du modèle avec la meilleure performance globale (exemple basé sur AUC-ROC)\n",
    "if lr_metrics[\"AUC-ROC\"] > rf_metrics[\"AUC-ROC\"]:\n",
    "    print(\"\\nLogistic Regression a performé le mieux selon AUC-ROC.\")\n",
    "    best_model = lr_model  # Assigner Logistic Regression comme meilleur modèle\n",
    "else:\n",
    "    print(\"\\nRandom Forest a performé le mieux selon AUC-ROC.\")\n",
    "    best_model = rf_model  # Assigner Random Forest comme meilleur modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interprétation du modèle AiOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Extraire les coefficients pour Logistic Regression\n",
    "coefficients = best_model.coef_[0]  # Coefficients du modèle Logistic Regression\n",
    "feature_names = X_train_full.columns  # Noms des variables dans l'ensemble combiné\n",
    "\n",
    "# Calculer l'importance des variables pour Logistic Regression\n",
    "logistic_importances = pd.DataFrame({\n",
    "    'Variable': feature_names,\n",
    "    'Importance': np.abs(coefficients)  # Valeurs absolues des coefficients\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Entraîner un modèle Random Forest pour comparer les importances des variables\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Calculer l'importance des variables pour Random Forest\n",
    "rf_importances = pd.DataFrame({\n",
    "    'Variable': feature_names,\n",
    "    'Importance': rf_model.feature_importances_  # Importances données par Random Forest\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Afficher les importances pour Logistic Regression\n",
    "print(\"\\nImportance des variables - Régression Logistique :\")\n",
    "print(logistic_importances)\n",
    "\n",
    "# Afficher les importances pour Random Forest\n",
    "print(\"\\nImportance des variables - Random Forest :\")\n",
    "print(rf_importances)\n",
    "\n",
    "# Visualisation des importances pour Logistic Regression\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(logistic_importances['Variable'], logistic_importances['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance (Magnitude des coefficients)')\n",
    "plt.ylabel('Variables')\n",
    "plt.title('Importance des Variables - Régression Logistique')\n",
    "plt.gca().invert_yaxis()  # Inverser l'ordre pour afficher les plus importantes en haut\n",
    "plt.show()\n",
    "\n",
    "# Visualisation des importances pour Random Forest\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(rf_importances['Variable'], rf_importances['Importance'], color='lightgreen')\n",
    "plt.xlabel('Importance (Score d\\'importance)')\n",
    "plt.ylabel('Variables')\n",
    "plt.title('Importance des Variables - Random Forest')\n",
    "plt.gca().invert_yaxis()  # Inverser l'ordre pour afficher les plus importantes en haut\n",
    "plt.show()\n",
    "\n",
    "# Comparaison des importances entre les deux modèles\n",
    "comparison = logistic_importances.merge(\n",
    "    rf_importances, on='Variable', suffixes=('_LogisticRegression', '_RandomForest')\n",
    ")\n",
    "\n",
    "# Afficher la comparaison\n",
    "print(\"\\nComparaison des importances des variables :\")\n",
    "print(comparison)\n",
    "\n",
    "# Visualisation de la comparaison\n",
    "plt.figure(figsize=(12, 8))\n",
    "comparison.plot(x='Variable', kind='bar', figsize=(12, 6))\n",
    "plt.title(\"Comparaison des Importances des Variables entre Régression Logistique et Random Forest\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.legend([\"Logistic Regression\", \"Random Forest\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
